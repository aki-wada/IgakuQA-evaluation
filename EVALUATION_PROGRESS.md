# IgakuQA LLM評価 経過記録

**記録日時**: 2026-02-06
**実行環境**: Mac Studio M3 Ultra
**評価対象**: 第116回医師国家試験 2022年 A問題 75問

**GitHubリポジトリ**: https://github.com/aki-wada/IgakuQA-evaluation

---

## v2プロンプト評価サマリー

### 評価完了モデル一覧（Best Accuracy順）

| Rank | Model | Size | Best Accuracy | Best Prompt | Avg Time |
|------|-------|------|---------------|-------------|----------|
| 1 | **qwen3-235b-a22b-2507** (MLX 8bit, mt=1024) | 235B MoE | **88.0%** ✓合格⭐⭐ | 案B/案C (同率) | 1.9s |
| 1 | **qwen3-235b-a22b** | 235B MoE | **88.0%** ✓合格⭐⭐ | 案B (段階的思考) | 1.4s |
| 1 | **gpt-oss-120b** (MLX 8bit, mt=1024) | 120B | **92.0%** ✓合格⭐⭐⭐ | 案A (回答形式強化) | 2.1s |
| - | **qwen3-next-80b** (MLX, mt=1024) | 80B MoE(A3B) | **85.3%** ✓合格⭐ | 案C (日本医療文脈) | 0.4s |
| 2 | **gpt-oss-120b** (GGUF MXFP4, mt=1024) | 120B | **90.7%** ✓合格⭐⭐⭐ | 案C (日本医療文脈) | 1.3s |
| 3 | **llama-3.3-swallow-70b** | 70B | **81.3%** ✓合格⭐ | Baseline/案B | 2.1s |
| 4 | **qwen3-vl-32b** | 32B VL | **82.7%** ✓合格⭐ | Baseline/案B | 3.7s |
| 5 | **qwen3-32b** | 32B | **80.0%** ✓合格⭐ | Baseline/案B | 1.5s |
| 6 | **mistral-large-2407** | 123B | **77.3%** ✓合格 | Baseline/案A/案C | 6.5s |
| 6 | medgemma-27b-text-it-mlx | 27B | **76.0%** ✓合格 | Baseline | 3.0s |
| 6 | mistral-small | - | **76.0%** ✓合格 | Baseline | - |
| 5 | qwen3-vl-30b | 30B | 74.7% | 案A/B/C同等 | 2.3s |
| 5 | gemma-3-27b | 27B | 74.7% | 案B (段階的思考) | 1.0s |
| 6 | **qwen3-14b** | 14B | 73.3% | 案A/B (+5.3%) | 0.7s |
| 7 | **gpt-oss-20b** (mt=1024) | 20B | **77.3%** ✓合格 | 案A (回答形式強化) | 1.2s |
| 7 | llama-3.3-70b-instruct | 70B | 68.0% | Baseline/案A/案B | 2.2s |
| 8 | shisa-v2-llama3.3-70b | 70B | 61.3% | 案A/案B | 2.2s |
| 8 | qwen3-8b | 8B | 61.3% | 案A (回答形式強化) | 0.4s |
| 8 | ezo2.5-gemma-3-12b | 12B | 60.0% | 案C (日本医療文脈) | 0.4s |
| 8 | qwen3-vl-8b | 8B | 60.0% | Baseline | 2.1s |
| 10 | phi-4 | 14B | 56.0% | 案A (回答形式強化) | - |
| 10 | gemma-3-12b-it | 12B | 54.7% | - | 1.6s |
| 10 | internvl3_5-8b | 8B | 54.7% | Baseline | 0.3s |
| 10 | qwen3-4b-2507 | 4B | 54.7% | 案C (日本医療文脈) | 0.8s |
| 13 | llama-3.1-swallow-8b@bf16 | 8B | 53.3% | 案B (段階的思考) | 0.4s |
| 13 | qwen3-vl-4b | 4B VL | 52.0% | 案B (段階的思考) | 2.0s |
| 14 | elyza-jp-8b | 8B | 44.0% | 案A (回答形式強化) | 0.3s |
| 15 | medgemma-4b@bf16 | 4B | 29.3% | 案A (回答形式強化) | 1.2s |
| 16 | medgemma-4b | 4B | 18.7% | - | 0.7s |
| 17 | lfm2.5-1.2b | 1.2B | 28.0% | 案A (回答形式強化) | 0.1s |

### 評価失敗モデル

| Model | 理由 |
|-------|------|
| glm-4.7-flash | 分析モード固定、全問で`a,c,d,e`出力（0%）→ [詳細診断](#glm-47-flash-詳細診断-2026-02-06) |
| glm-4.6v-flash-mlx | 中国語出力（2.7%）- 日本語QA不適 |
| nvidia/nemotron-3-nano | 全問で複数選択肢列挙（0-1.3%） |
| fallen-command-a-111b | 全問でd,e含む複数選択肢列挙（1.3%）、3bit量子化が原因の可能性 |
| internlm3-8b-instruct | API不安定、16% |
| microsoft/phi-4-reasoning-plus | API error - モデル未ロード |
| internvl3-14b | API error - 全リクエスト失敗 |
| plamo13B | API error - 全リクエスト失敗 |

### 合格ライン達成（75%以上）
- **gpt-oss-120b** MLX 8bit mt=1024 (92.0%) ⭐⭐⭐全モデル最高スコア
- **gpt-oss-120b** GGUF MXFP4 mt=1024 (90.7%) ⭐⭐⭐
- **qwen3-235b-a22b-2507** MLX 8bit mt=1024 (88.0%) ⭐⭐2507版、Baselineが+6.7%安定化
- **qwen3-235b-a22b** (88.0%) ⭐⭐GPT-4超え
- **llama-3.3-swallow-70b** (81.3%) ⭐日本語ファインチューン70B
- **qwen3-vl-32b** (82.7%) ⭐VLモデル、qwen3-32bの上位互換
- **qwen3-32b** (80.0%) ⭐GPT-4レベル
- **qwen3-next-80b** mt=1024 (85.3%) ⭐MoE Active 3B、超高速0.4s、Baselineが66.7%と低いがプロンプトで+18.7%改善
- **gpt-oss-20b** mt=1024 (77.3%) reasoningモデル、mt=50では32%→mt=1024で合格
- **mistral-large-2407** (77.3%) 123Bモデル
- **medgemma-27b-text-it-mlx** (76.0%)
- **mistral-small** (76.0%)

### ChatGPT (58%) 超え
- qwen3-vl-30b (74.7%)
- llama-3.3-70b-instruct (68.0%)
- ezo2.5-gemma-3-12b (60.0%) 🇯🇵日本語FT
- qwen3-vl-8b (60.0%)

### 最適プロンプト傾向
| モデルタイプ | 最適プロンプト |
|-------------|---------------|
| 大規模(27B+) | Baseline |
| 中規模(14-30B) | 案B (段階的思考) |
| 小規模(<10B) | 案A/案C |

### ベースライン比較（論文より）
| Model | Accuracy | 備考 |
|-------|----------|------|
| GPT-4 | 80.0% | クラウドAPI |
| ChatGPT | 58.0% | クラウドAPI |
| GPT-3 | 42.0% | クラウドAPI |

---

## 使用プロンプト詳細

### v1→v2変更経緯

初回評価（v1）では以下のプロンプトを使用:
```
System: あなたは医師国家試験を解く専門家です。選択肢から正解をa,b,c,d,eで答えてください。
```
**注意**: 「医師国家試験」という文言はベンチマーク汚染（data contamination）のリスクがあるため、v2で試験名を含まないプロンプトに変更した。

### Baseline (現行・v2)
```
System: あなたは医学の専門家です。選択肢から正解をa,b,c,d,eで答えてください。
max_tokens: 50
```

### 案A: 回答形式強化
```
System: あなたは臨床医学に精通した医師です。
問題を読み、正解の選択肢をアルファベット(a,b,c,d,e)のみで回答してください。
複数選択の場合はカンマ区切りで回答してください。例: a,c
余計な説明は不要です。
max_tokens: 50
```

### 案B: 段階的思考
```
System: あなたは臨床医学に精通した医師です。

回答手順:
1. 問題文のキーワードを確認
2. 各選択肢を医学的に検討
3. 正解を選択

最終行に「答え:」に続けて選択肢(a,b,c,d,e)のみを記載してください。
max_tokens: 200
```

### 案C: 日本医療文脈
```
System: あなたは日本の医療制度と臨床医学に精通した専門医です。
医学の多肢選択問題に回答します。

重要:
- 日本の診療ガイドラインに基づいて判断
- 選択肢はa,b,c,d,eのアルファベットで回答
- 指定された個数を必ず選択

回答形式: a または a,b,c（複数の場合）
max_tokens: 50
```

### qwen3モデル専用対応
```
# システムプロンプト末尾に自動追加
/no_think

# 回答から<think>タグを除去
response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL).strip()
```

### 共通設定
- temperature: 0
- Few-shot: 2例（同一年度の別問題）
- 問題形式: 「N個選んで答えてください。」

---

### glm-4.7-flash 詳細診断 (2026-02-06)

**問題**: モデルが常に `1.  **Analyze the User's Request:**` から始まる分析モードで出力し、実際の回答(a/b/c/d/e)に到達しない。

**テスト結果**: 全11パターンで同一の挙動

| テスト設定 | 結果 |
|-----------|------|
| System prompt あり/なし | ❌ 変化なし |
| 英語プロンプト | ❌ 変化なし |
| 中国語プロンプト | ❌ 変化なし |
| max_tokens=5〜200 | ❌ 変化なし |
| temperature=0, 0.1 | ❌ 変化なし |
| stop tokens (`\n`, `。`, `、`) | ❌ 変化なし |
| 思考禁止指示 | ❌ 変化なし |
| アシスタントプレフィル | ❌ 変化なし |

**出力例**:
```
1.  **Analyze the User's Request:**
    *   **Task:** Answer a multiple-choice question about medical ethics.
    *   **Question:** Which statement regarding the professional ethics...
```

**結論**: モデルのファインチューニング時に分析・推論モードがハードコードされており、OpenAI互換APIの設定では変更不可。IgakuQA評価に使用不能。

---

## 未評価モデル（LM Studioで利用可能）

**最終更新**: 2026-02-07

※ 評価済みモデルの結果は「v2プロンプト評価サマリー」を参照

### 大規模モデル（70B+）
```
llama-3.1-swallow-70b-instruct-v0.3
meta/llama-3.3-70b
nousresearch/hermes-4-70b
```

### 中規模モデル（10B-30B）
```
qwen/qwen3-30b-a3b
qwen2.5-14b-instruct-mlx
qwen2.5-32b-instruct-mlx
llama-4-scout-17b-16e-mlx-text
llm-jp-3.1-13b-instruct4
elyza-japanese-llama-2-13b-fast-instruct
```

### 小規模モデル（<10B）
```
google/gemma-3-4b
google/gemma-3-1b
llama-3.2-3b-instruct
internvl3-8b@bf16
internvl3_5-4b
llama-3-swallow-8b-v0.1
tanuki-8b-dpo-v1.0
```

### 日本語特化モデル
```
llama-3-swallow-8b-v0.1
llama-3.1-swallow-70b-instruct-v0.3
llm-jp-3.1-13b-instruct4
tanuki-8b-dpo-v1.0
plamo-2-translate
```

---

## Mac Studio M3 Ultraでの継続手順

### 0. リポジトリをクローン

```bash
git clone https://github.com/aki-wada/IgakuQA-evaluation.git
cd IgakuQA-evaluation
```

### 1. 環境セットアップ

```bash
# 仮想環境作成
python3 -m venv venv
source venv/bin/activate
pip install requests
```

### 2. LM Studio起動確認

```bash
# モデル一覧確認
curl -s http://localhost:1234/v1/models | python3 -c "import sys,json; data=json.load(sys.stdin); print('\n'.join([m['id'] for m in data['data']]))"
```

### 3. 評価実行コマンド

#### 単一モデル評価
```bash
python evaluate_lmstudio_batch.py \
  --models "モデル名" \
  --year 2022 \
  --section A \
  --use-few-shot \
  --output results/モデル名_evaluation.json
```

#### 複数モデル一括評価
```bash
python evaluate_lmstudio_batch.py \
  --models "model1" "model2" "model3" \
  --year 2022 \
  --section A \
  --use-few-shot \
  --output results/batch_evaluation.json
```

#### 全セクション評価（medgemma-27bで合否判定）
```bash
for section in A B C D E F; do
  python evaluate_lmstudio_batch.py \
    --models "medgemma-27b-text-it-mlx" \
    --year 2022 \
    --section $section \
    --use-few-shot \
    --output results/medgemma-27b_2022_${section}.json
done
```

### 4. おすすめ追加評価

M3 Ultraの192GB RAM環境では以下が推奨：

1. **大規模モデル追加評価**
   - qwen3-vl-30b
   - translategemma-27b-it
   - internvl3_5-14b

2. **日本語特化モデル再評価**
   - llama-3-elyza-jp-8b-mlx（要モデルロード）
   - ezo2.5-gemma-3-12b-it-preview

3. **medgemma-27bで全セクション評価**
   - A〜F全問で総合合否判定

---

## 結果ファイル一覧

```
results/
├── medgemma_evaluation.json      # medgemma 3モデル
├── qwen3_evaluation.json         # qwen3-4b
├── additional_evaluation.json    # gemma-3-12b等
├── gpt-oss-20b_evaluation.json   # gpt-oss-20b
```

---

### プロンプト比較実験結果

**実施日**: 2026-02-02

#### medgemma-27b-text-it-mlx（75問）

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | **Baseline (現行)** | **76.0%** | 57/75 | - |
| 2 | 案C: 日本医療文脈 | 74.7% | 56/75 | -1.3% |
| 3 | 案A: 回答形式強化 | 70.7% | 53/75 | -5.3% |
| 4 | 案B: 段階的思考 | 56.0% | 42/75 | **-20.0%** |

#### qwen/qwen3-4b-2507（75問）

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | **案C: 日本医療文脈** | **54.7%** | 41/75 | **+2.7%** |
| 2 | Baseline (現行) | 52.0% | 39/75 | - |
| 2 | 案B: 段階的思考 | 52.0% | 39/75 | ±0% |
| 4 | 案A: 回答形式強化 | 50.7% | 38/75 | -1.3% |

#### openai/gpt-oss-120b MLX 8bit（75問）⭐⭐最高スコア同率

| Rank | プロンプト | 正答率 | 正解数 | 空回答 | ベースライン比 |
|------|-----------|--------|--------|--------|---------------|
| 1 | **案B: 段階的思考** | **88.0%** | 66/75 | 2 | **+61.3%** |
| 2 | 案A: 回答形式強化 | 30.7% | 23/75 | 49 | +4.0% |
| 3 | Baseline (現行) | 26.7% | 20/75 | 46 | - |
| 3 | 案C: 日本医療文脈 | 26.7% | 20/75 | 48 | ±0% |

**発見**:
- 120Bモデルで案B（max_tokens=200）のみ88.0%達成
- gpt-ossシリーズのスケーリング: 20B(68.0%) → **120B(88.0%)** で+20%向上
- 案B（段階的思考, max_tokens=200）でベースラインから**+61.3%**の劇的改善
- 他プロンプトでは空回答が多発（46-49件）、案Bで2件に激減
- Avg Time: 2.2s

**max_tokens=1024（全プロンプト統一）⭐⭐⭐全モデル最高スコア**

| Rank | プロンプト | 正答率 | 正解数 | 空回答 | ベースライン比 |
|------|-----------|--------|--------|--------|---------------|
| 1 | **案A: 回答形式強化** | **92.0%** | 69/75 | 0 | **+6.7%** |
| 2 | **案B: 段階的思考** | **90.7%** | 68/75 | 0 | +5.3% |
| 3 | **案C: 日本医療文脈** | **89.3%** | 67/75 | 0 | +4.0% |
| 4 | Baseline (現行) | 85.3% | 64/75 | 0 | - |

**重大発見**:
- **案A + max_tokens=1024 で92.0%達成！全モデル・全条件で最高スコア⭐⭐⭐**
- MLX 8bit版（124.20GB）はGGUF MXFP4版（63.39GB）を最高スコアで上回った: 92.0% vs 90.7%
- max_tokens=50: 空回答46-49件(26-30%) → **1024: 0件**（完全解消）
- **案Aが最適**: MLX 8bit版ではシンプルな回答形式強化が最も有効
- 全プロンプトで合格ライン超え（85.3%-92.0%）
- Avg Time: 2.1-2.3s

**GGUF MXFP4 vs MLX 8bit 比較（max_tokens=1024）**

| プロンプト | GGUF MXFP4 | MLX 8bit | 差分 | 勝者 |
|-----------|-----------|---------|------|------|
| Baseline | 88.0% | 85.3% | -2.7% | GGUF |
| **案A** | 85.3% | **92.0%** | **+6.7%** | **MLX** |
| 案B | 88.0% | 90.7% | +2.7% | MLX |
| 案C | **90.7%** | 89.3% | -1.4% | GGUF |
| **Best** | **90.7%** | **92.0%** | **+1.3%** | **MLX** |

- MLX 8bit版が案A/案Bで優位、GGUF MXFP4版がBaseline/案Cで優位
- 最高スコアではMLX 8bit(92.0%) > GGUF MXFP4(90.7%)
- **メモリ使用量**: MLX 8bit 124.20GB vs GGUF MXFP4 63.39GB → MLXは2倍のメモリだが1.3%高い
- **速度**: MLX 2.1s vs GGUF 1.3s → GGUFの方が高速

#### openai/gpt-oss-120b GGUF MXFP4（75問）⭐⭐⭐最高スコア更新、max_tokens実験

**max_tokens=50（デフォルト）**

| Rank | プロンプト | 正答率 | 正解数 | 空回答 | ベースライン比 |
|------|-----------|--------|--------|--------|---------------|
| 1 | **案B: 段階的思考** | **85.3%** | 64/75 | 4 | **+56.0%** |
| 2 | 案C: 日本医療文脈 | 33.3% | 25/75 | 43 | +4.0% |
| 3 | Baseline (現行) | 29.3% | 22/75 | 49 | - |
| 4 | 案A: 回答形式強化 | 26.7% | 20/75 | 49 | -2.7% |

**max_tokens=200（全プロンプト統一）**

| Rank | プロンプト | 正答率 | 正解数 | 空回答 | ベースライン比 |
|------|-----------|--------|--------|--------|---------------|
| 1 | **案B: 段階的思考** | **85.3%** | 64/75 | 4 | +1.3% |
| 2 | Baseline (現行) | 84.0% | 63/75 | 5 | - |
| 2 | 案C: 日本医療文脈 | 84.0% | 63/75 | 6 | ±0% |
| 4 | 案A: 回答形式強化 | 82.7% | 62/75 | 5 | -1.3% |

**max_tokens=1024（全プロンプト統一）⭐⭐⭐最高スコア**

| Rank | プロンプト | 正答率 | 正解数 | 空回答 | ベースライン比 |
|------|-----------|--------|--------|--------|---------------|
| 1 | **案C: 日本医療文脈** | **90.7%** | 68/75 | 0 | **+2.7%** |
| 2 | **Baseline (現行)** | **88.0%** | 66/75 | 0 | - |
| 2 | **案B: 段階的思考** | **88.0%** | 66/75 | 0 | ±0% |
| 4 | 案A: 回答形式強化 | 85.3% | 64/75 | 0 | -2.7% |

**重大発見**:
- **案C + max_tokens=1024 で90.7%達成！全モデル・全条件で最高スコア⭐⭐⭐**
- **max_tokensの影響が極めて大きい**: 案C 33.3%(mt=50) → 84.0%(mt=200) → **90.7%**(mt=1024)
- gpt-oss-120bはreasoningモデル → 内部推論(reasoning)がmax_tokensを消費し、出力(content)に回答が出ない
- max_tokens=50: 空回答43-49件 → 200: 4-6件 → **1024: 0件**（完全解消）
- **GGUF MXFP4版がMLX 8bit版(88.0%)を2.7%上回った** → 量子化形式の差でなくmax_tokensが支配的要因
- 全プロンプトで合格ライン超え（85.3%-90.7%）
- **案C（日本医療文脈）が最適プロンプト** → reasoningモデルでは日本の診療ガイドラインへの言及が有効
- Avg Time: 1.3s（MLX版2.2sより高速）

#### openai/gpt-oss-20b（75問）⭐重大発見、max_tokens実験

**max_tokens=50/200（デフォルト）**

| Rank | プロンプト | 正答率 | 正解数 | 空回答 | ベースライン比 |
|------|-----------|--------|--------|--------|---------------|
| 1 | **案B: 段階的思考** | **68.0%** | 51/75 | 5 | **+36.0%** |
| 2 | 案C: 日本医療文脈 | 33.3% | 25/75 | 45 | +1.3% |
| 3 | Baseline (現行) | 32.0% | 24/75 | 42 | - |
| 4 | 案A: 回答形式強化 | 26.7% | 20/75 | 44 | -5.3% |

**max_tokens=1024（全プロンプト統一）✓合格ライン達成**

| Rank | プロンプト | 正答率 | 正解数 | 空回答 | ベースライン比 |
|------|-----------|--------|--------|--------|---------------|
| 1 | **案A: 回答形式強化** | **77.3%** | 58/75 | 0 | **+2.7%** |
| 2 | 案B: 段階的思考 | 76.0% | 57/75 | 0 | +1.3% |
| 3 | Baseline (現行) | 74.7% | 56/75 | 0 | - |
| 4 | 案C: 日本医療文脈 | 69.3% | 52/75 | 0 | -5.3% |

**重大発見**:
- **案A + max_tokens=1024 で77.3%達成、合格ライン超え！✓**
- 案B（段階的思考, max_tokens=200）で以前68.0%だったが、全プロンプトmt=1024で更に改善
- **max_tokensの影響が極めて大きい**: 案A 26.7%(mt=50) → **77.3%**(mt=1024) で**+50.6%**
- mt=50: 空回答42-45件 → **mt=1024: 0件**（完全解消）
- gpt-oss-20bもreasoningモデル → 120Bと同じmax_tokens問題が存在
- **gpt-ossスケーリング(mt=1024)**: 20B(77.3%) → 120B GGUF(90.7%) → 120B MLX(92.0%)
- Avg Time: 1.2-1.3s
- API error 1件（案C、116A73）

#### mlx-community/llama-3.3-70b-instruct（75問）

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | **Baseline (現行)** | **68.0%** | 51/75 | - |
| 1 | **案A: 回答形式強化** | **68.0%** | 51/75 | ±0% |
| 1 | **案B: 段階的思考** | **68.0%** | 51/75 | ±0% |
| 4 | 案C: 日本医療文脈 | 66.7% | 50/75 | -1.3% |

**発見**:
- 70Bモデルで68.0%達成、gpt-oss-20bと同等
- プロンプトによる差が小さい（全て±1.3%以内）
- 空回答なし、安定した出力
- 4bit量子化版（39.71GB）使用
- Avg Time: 2.1-2.4s
- 合格ライン（75%）には届かず

#### tokyotech-llm/llama-3.3-swallow-70b-instruct-v0.4（75問）⭐日本語ファインチューン

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | **Baseline (現行)** | **81.3%** | 61/75 | - |
| 1 | **案B: 段階的思考** | **81.3%** | 61/75 | ±0% |
| 3 | 案A: 回答形式強化 | 77.3% | 58/75 | -4.0% |
| 3 | 案C: 日本医療文脈 | 77.3% | 58/75 | -4.0% |

**発見**:
- **東京工業大学の日本語ファインチューンにより81.3%達成、合格ライン超え⭐**
- ベースllama-3.3-70b-instruct（68.0%）から**+13.3%**の大幅改善
- 日本語ファインチューニングの効果が顕著
- Baseline/案Bで同率最高（シンプルなプロンプトが効果的）
- 案A/案Cでは-4%低下（制約追加で逆効果）
- Avg Time: 2.1s
- 4bit量子化版使用

#### mistral-large-instruct-2407（75問）✓合格

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | **Baseline (現行)** | **77.3%** | 58/75 | - |
| 1 | **案A: 回答形式強化** | **77.3%** | 58/75 | ±0% |
| 1 | **案C: 日本医療文脈** | **77.3%** | 58/75 | ±0% |
| 4 | 案B: 段階的思考 | 73.3% | 55/75 | -4.0% |

**発見**:
- **123Bモデルで77.3%達成、合格ライン超え✓**
- Baseline/案A/案Cで同率最高（大規模モデルはシンプルなプロンプトが効果的）
- 案B（段階的思考）では-4.0%低下 → max_tokens=200で逆にノイズが増加
- **Mistralスケーリング**: small(76.0%) → **Large 123B(77.3%)** で+1.3%のみ、コスパは悪い
- MLX 8bit版（130.28GB）使用
- Avg Time: 6.5-8.2s（123Bモデルのため推論が遅い）
- 空回答なし、安定出力

#### ezo2.5-gemma-3-12b-it-preview（75問）🇯🇵EZO日本語FT

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | **案C: 日本医療文脈** | **60.0%** | 45/75 | **+2.7%** |
| 2 | 案A: 回答形式強化 | 58.7% | 44/75 | +1.3% |
| 3 | Baseline (現行) | 57.3% | 43/75 | - |
| 3 | 案B: 段階的思考 | 57.3% | 43/75 | ±0% |

**発見**:
- gemma-3-12b-itの日本語FT版（AXCXEPT社EZOプロジェクト）で60.0%達成、**ChatGPT超え**
- ベースgemma-3-12b-it（54.7%）から**+5.3%**改善 → 日本語FT効果あり
- **案C（日本医療文脈）が最良** → 日本語FTモデルに日本語文脈プロンプトが有効
- Q4_K_S量子化版（6.94GB）でも安定動作
- Avg Time: 0.4s と高速推論
- 空回答なし、安定出力
- 12Bでqwen3-vl-8b（60.0%）と同等の結果

#### llama-3.1-swallow-8b-instruct-v0.5@bf16（75問）🇯🇵Swallowシリーズ8B

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | **案B: 段階的思考** | **53.3%** | 40/75 | **+5.3%** |
| 2 | 案A: 回答形式強化 | 52.0% | 39/75 | +4.0% |
| 2 | 案C: 日本医療文脈 | 52.0% | 39/75 | +4.0% |
| 4 | Baseline (現行) | 48.0% | 36/75 | - |

**発見**:
- Swallowシリーズ8B（bf16精度）で53.3%達成
- elyza-jp-8b（44.0%）を**+9.3%**上回る → Swallowの日本語FTがより効果的
- ただしqwen3-vl-8b（60.0%）には及ばず（-6.7%）
- **Swallowスケーリング**: 8B(53.3%) → 70B(81.3%) で**+28.0%**の大幅スケーリング
- 案B（段階的思考）で+5.3%改善、中規模モデルの傾向に一致
- Avg Time: 0.4s と非常に高速（bf16精度でも高速推論）
- 空回答なし、安定出力

#### medgemma-1.5-4b-it@bf16（75問）

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | **案A: 回答形式強化** | **29.3%** | 22/75 | **+8.0%** |
| 2 | 案C: 日本医療文脈 | 26.7% | 20/75 | +5.3% |
| 3 | 案B: 段階的思考 | 25.3% | 19/75 | +4.0% |
| 4 | Baseline (現行) | 21.3% | 16/75 | - |

**発見**: 小規模医療特化モデルでは案A（回答形式強化）が効果的。ただし全体的に低精度。

#### mistralai/magistral-small-2509（75問）

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | **Baseline (現行)** | **76.0%** | 57/75 | - |
| 1 | **案B: 段階的思考** | **76.0%** | 57/75 | ±0% |
| 3 | 案A: 回答形式強化 | 74.7% | 56/75 | -1.3% |
| 4 | 案C: 日本医療文脈 | 73.3% | 55/75 | -2.7% |

**発見**: 76%でmedgemma-27bと同等の高精度。プロンプト変更による改善なし。

#### phi-4（75問）

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | **案A: 回答形式強化** | **56.0%** | 42/75 | **+12.0%** |
| 2 | 案C: 日本医療文脈 | 54.7% | 41/75 | +10.7% |
| 3 | 案B: 段階的思考 | 49.3% | 37/75 | +5.3% |
| 4 | Baseline (現行) | 44.0% | 33/75 | - |

**発見**: プロンプト最適化の効果が顕著。案Aで+12%改善。14Bで56%達成。

#### glm-4.6v-flash-mlx（75問）⚠️日本語QA非対応

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | 案A: 回答形式強化 | 2.7% | 2/75 | +2.7% |
| 1 | 案C: 日本医療文脈 | 2.7% | 2/75 | +2.7% |
| 3 | Baseline (現行) | 0.0% | 0/75 | - |
| 3 | 案B: 段階的思考 | 0.0% | 0/75 | ±0% |

**問題点**:
- 出力が中国語になる（`<think>用户现`等）
- 日本語プロンプトを正しく理解できない
- 複数のAPI errorが発生
- **結論**: 日本語医学QAには不適

#### qwen/qwen3-32b（75問）⭐GPT-4レベル達成

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | **Baseline (現行)** | **80.0%** | 60/75 | - |
| 1 | **案B: 段階的思考** | **80.0%** | 60/75 | ±0% |
| 3 | 案A: 回答形式強化 | 76.0% | 57/75 | -4.0% |
| 3 | 案C: 日本医療文脈 | 76.0% | 57/75 | -4.0% |

**発見**:
- 32Bモデルで80.0%達成、GPT-4 (80%)レベルに到達⭐
- **全プロンプトで合格ライン（75%）超え**
- シンプルなBaselineが最良（大規模モデルの傾向）
- qwen3シリーズのスケーリング: 4B(54.7%) → 8B(60.0%) → 30B(74.7%) → 32B(80.0%) → **235B(88.0%)**
- **重要**: qwen3の`/no_think`指示が必須（<think>タグ除去で対応）
- Avg Time: 1.5s と高速推論

#### qwen/qwen3-235b-a22b（75問）⭐⭐最高スコア

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | **案B: 段階的思考** | **88.0%** | 66/75 | **+8.0%** |
| 2 | 案A: 回答形式強化 | 86.7% | 65/75 | +6.7% |
| 2 | 案C: 日本医療文脈 | 86.7% | 65/75 | +6.7% |
| 4 | Baseline (現行) | 80.0% | 60/75 | - |

**発見**:
- **235B MoE（22Bアクティブ）モデルで88.0%達成、最高スコア⭐⭐**
- GPT-4（80%）を大幅に上回る性能
- 案B（段階的思考）でベースラインから+8.0%改善
- **全プロンプトで合格ライン（75%）超え**
- `/no_think`指示により、以前のthinking出力問題を解決
- qwen3スケーリング: 4B(54.7%) → 8B(60.0%) → 14B(73.3%) → 30B(74.7%) → 32B(80.0%) → **235B(88.0%)**
- Avg Time: 1.4s（MoEによる効率的推論）

#### qwen/qwen3-235b-a22b-2507 MLX 8bit（75問）⭐⭐2507版、mt=1024

| Rank | プロンプト | 正答率 | 正解数 | 空回答 | ベースライン比 |
|------|-----------|--------|--------|--------|---------------|
| 1 | **案B: 段階的思考** | **88.0%** | 66/75 | 0 | **+1.3%** |
| 1 | **案C: 日本医療文脈** | **88.0%** | 66/75 | 0 | **+1.3%** |
| 3 | Baseline (現行) | 86.7% | 65/75 | 0 | - |
| 3 | 案A: 回答形式強化 | 86.7% | 65/75 | 0 | ±0% |

**旧版 qwen3-235b-a22b（132.26GB）との比較**

| プロンプト | 旧版 (132GB) | 2507版 (250GB) | 差分 |
|-----------|-------------|---------------|------|
| Baseline | 80.0% | **86.7%** | **+6.7%** |
| 案A | 86.7% | 86.7% | ±0% |
| 案B | 88.0% | **88.0%** | ±0% |
| 案C | 86.7% | **88.0%** | **+1.3%** |

**発見**:
- **2507版（Instruct, MLX 8bit, 249.80GB）でBest 88.0%、旧版と同率**
- **Baselineが80.0% → 86.7%と+6.7%大幅改善** → 2507版はプロンプトへの依存度が大幅に低下
- 全プロンプトで86.7-88.0%と極めて安定（旧版は80.0-88.0%で8%のばらつき → 2507版は1.3%）
- 空回答ゼロ、API errorゼロ、完璧な安定動作
- `/no_think`指示 + max_tokens=1024 で運用
- Avg Time: 1.9s（旧版1.4sよりやや遅い、249.80GBモデルのため）
- **メモリ使用量**: 旧版 132.26GB vs 2507版 249.80GB → メモリ2倍だがBestスコアは同じ
- **結論**: Bestスコア重視なら旧版(132GB)で十分、安定性重視なら2507版(250GB)が優位

#### qwen/qwen3-14b（75問）

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | **案A: 回答形式強化** | **73.3%** | 55/75 | **+5.3%** |
| 1 | **案B: 段階的思考** | **73.3%** | 55/75 | **+5.3%** |
| 3 | 案C: 日本医療文脈 | 72.0% | 54/75 | +4.0% |
| 4 | Baseline (現行) | 68.0% | 51/75 | - |

**発見**:
- 14Bモデルで73.3%達成、惜しくも合格ライン（75%）に届かず
- 案A/案Bでベースラインから+5.3%改善（中規模モデルの傾向に一致）
- qwen3スケーリング: 4B(54.7%) → 8B(60.0%) → **14B(73.3%)** → 30B(74.7%) → 32B(80.0%) → 235B(88.0%)
- Avg Time: 0.7s と非常に高速

#### qwen/qwen3-vl-30b（75問）

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | 案A: 回答形式強化 | **74.7%** | 56/75 | **+2.7%** |
| 1 | 案B: 段階的思考 | **74.7%** | 56/75 | **+2.7%** |
| 1 | 案C: 日本医療文脈 | **74.7%** | 56/75 | **+2.7%** |
| 4 | Baseline (現行) | 72.0% | 54/75 | - |

**発見**:
- 30B VLモデルで74.7%達成、惜しくも合格ラインに届かず
- 全ての最適化プロンプトで同等の性能（+2.7%改善）
- qwen3-4b（54.7%）から+20%の大幅改善でスケーリング効果が顕著
- Avg Time: 2.3s で高速推論

#### elyza/llama-3-elyza-jp-8b（75問）🇯🇵日本語特化

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | **案A: 回答形式強化** | **44.0%** | 33/75 | **+4.0%** |
| 2 | 案C: 日本医療文脈 | 42.7% | 32/75 | +2.7% |
| 3 | 案B: 段階的思考 | 41.3% | 31/75 | +1.3% |
| 4 | Baseline (現行) | 40.0% | 30/75 | - |

**発見**:
- 日本語特化8Bモデルとして評価（LM Studio CLIで自動ロード成功）
- 案A（回答形式強化）で+4%改善
- Avg Time: 0.3s と非常に高速
- 同サイズのqwen3-8bとの比較が今後必要
- Llama 3ベースの日本語モデルは期待より低い結果

#### qwen/qwen3-vl-8b（75問）

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | **Baseline (現行)** | **60.0%** | 45/75 | - |
| 1 | **案B: 段階的思考** | **60.0%** | 45/75 | ±0% |
| 1 | **案C: 日本医療文脈** | **60.0%** | 45/75 | ±0% |
| 4 | 案A: 回答形式強化 | 58.7% | 44/75 | -1.3% |

**発見**:
- 8B VLモデルで60.0%達成、ChatGPT (58%)を超える好成績
- 全プロンプトで安定した結果（±1.3%以内）
- 同サイズのelyza-jp-8b（44.0%）を+16%上回る
- qwen3シリーズのスケーリング: 4B(54.7%) → 8B(60.0%) → 30B(74.7%)
- Avg Time: 2.1s

#### internvl3_5-8b（75問）

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | **Baseline (現行)** | **54.7%** | 41/75 | - |
| 2 | 案B: 段階的思考 | 53.3% | 40/75 | -1.3% |
| 3 | 案A: 回答形式強化 | 52.0% | 39/75 | -2.7% |
| 3 | 案C: 日本医療文脈 | 52.0% | 39/75 | -2.7% |

**発見**:
- 8B VLモデルで54.7%達成（gemma-3-12bと同等）
- シンプルなBaselineが最良、プロンプト最適化で逆に低下
- 同サイズのqwen3-vl-8b（60.0%）より5.3%低い
- Avg Time: 0.3s と非常に高速

#### qwen/qwen3-8b（75問）

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | **案A: 回答形式強化** | **61.3%** | 46/75 | **+6.7%** |
| 2 | 案C: 日本医療文脈 | 58.7% | 44/75 | +4.0% |
| 3 | Baseline (現行) | 54.7% | 41/75 | - |
| 4 | 案B: 段階的思考 | 6.7% | 5/75 | **-48.0%** |

**発見**:
- 案A（回答形式強化）で61.3%達成、ChatGPT(58%)超え
- **案Bが壊滅（6.7%）**: 「問題文のキーワードを」とプロンプト指示文をそのまま出力する問題が発生
- qwen3-vl-8b（60.0%）とほぼ同等（+1.3%）
- **qwen3スケーリング**: 4B(54.7%) → **8B(61.3%)** → 14B(73.3%) → 32B(80.0%) → 235B(88.0%)
- 案A（回答形式強化）で+6.7%改善、小規模モデルに有効な傾向
- `/no_think`指示と`<think>`タグ除去を適用
- Avg Time: 0.4s と非常に高速
- 空回答なし（案B除く）

#### google/gemma-3-27b（75問）惜しい！合格ラインまで0.3%

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | **案B: 段階的思考** | **74.7%** | 56/75 | **+1.3%** |
| 2 | Baseline (現行) | 73.3% | 55/75 | - |
| 2 | 案A: 回答形式強化 | 73.3% | 55/75 | ±0% |
| 4 | 案C: 日本医療文脈 | 70.7% | 53/75 | -2.7% |

**発見**:
- 27Bモデルで74.7%達成、**合格ライン（75%）まであと0.3%（1問差）**
- **gemmaスケーリング**: 12B(54.7%) → **27B(74.7%)** で**+20.0%**の大幅改善
- **medgemma-27bとの比較**: gemma-3-27b(74.7%) vs medgemma-27b(76.0%) → 医療特化FTの効果は**わずか+1.3%**
- 案B（段階的思考）でわずかに改善、案C（日本医療文脈）は逆効果
- Q4_K_S量子化版（16.87GB）使用
- Avg Time: 1.0s と高速推論
- API error 1件あり（116A45）、空回答なし
- gemmaシリーズ: 12B(54.7%) → 27B(74.7%) → medgemma-27b(76.0%)

#### shisa-v2-llama3.3-70b（75問）🇯🇵Shisa日本語FT

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | **案A: 回答形式強化** | **61.3%** | 46/75 | **+2.7%** |
| 1 | **案B: 段階的思考** | **61.3%** | 46/75 | **+2.7%** |
| 3 | 案C: 日本医療文脈 | 60.0% | 45/75 | +1.3% |
| 4 | Baseline (現行) | 58.7% | 44/75 | - |

**発見**:
- Shisa AI の日本語・英語バイリンガルFTモデル（llama-3.3-70b ベース）で61.3%
- **ベースllama-3.3-70b-instruct（68.0%）から-6.7%低下** → 日本語FTが逆効果
- **Swallow-70b（81.3%）とは-20%の大差** → 同じベースでもFT手法で大きく異なる
- 日本語FT 70Bモデル比較: **Swallow(81.3%) >> base(68.0%) > Shisa(61.3%)**
- プロンプトによる差は小さい（58.7%-61.3%、±2.7%以内）
- 4bit量子化版（39.71GB）使用
- Avg Time: 2.1-2.6s
- 空回答なし、安定出力

#### nvidia/nemotron-3-nano（75問）⚠️指示追従不能

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | 案B: 段階的思考 | 1.3% | 1/75 | +1.3% |
| 2 | Baseline (現行) | 0.0% | 0/75 | - |
| 2 | 案A: 回答形式強化 | 0.0% | 0/75 | ±0% |
| 2 | 案C: 日本医療文脈 | 0.0% | 0/75 | ±0% |

**問題点**:
- 全問で`a,c,d,e`や`a,b,c,d,e`のように**複数選択肢を列挙**する異常出力
- 1つの選択肢を選ぶ指示に従えず、ほぼ全選択肢を返す
- glm-4.7-flashと同様の指示追従問題
- 30B（33.58GB）のサイズにもかかわらず、日本語医学QAには完全に不適
- **結論**: nemotron_hアーキテクチャはOpenAI互換APIでの単純QAに不向き

#### fallen-command-a-111b-v1.1-mlx（75問）⚠️指示追従不能

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | Baseline (現行) | 1.3% | 1/75 | - |
| 1 | 案A: 回答形式強化 | 1.3% | 1/75 | ±0% |
| 1 | 案B: 段階的思考 | 1.3% | 1/75 | ±0% |
| 1 | 案C: 日本医療文脈 | 1.3% | 1/75 | ±0% |

**問題点**:
- 全問で`d,e`や`x,d,e`のように**常にd,eを含む複数選択肢を列挙**する異常出力
- 正解をd,eの組み合わせに含んでいるケースも多いが、余分な選択肢が付加される
- 全4プロンプトで完全に同一の挙動（1/75正解、116A68のd,eが偶然一致）
- Cohere2アーキテクチャ（111B）、MLX 3bit量子化版（48.61GB）
- **3bit量子化が原因の可能性** → 極端な量子化でモデルの指示追従能力が損なわれた可能性
- nemotron-3-nano、glm-4.7-flashに続く3例目の指示追従不能モデル
- **結論**: IgakuQA評価に使用不能

#### qwen/qwen3-vl-32b（75問）⭐合格、VLモデル

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | **Baseline (現行)** | **82.7%** | 62/75 | - |
| 1 | **案B: 段階的思考** | **82.7%** | 62/75 | ±0% |
| 3 | 案A: 回答形式強化 | 81.3% | 61/75 | -1.3% |
| 3 | 案C: 日本医療文脈 | 81.3% | 61/75 | -1.3% |

**発見**:
- **32B VLモデルで82.7%達成、合格ライン超え⭐ 全プロンプトで合格**
- **qwen3-32b（80.0%）から+2.7%改善** → VL版の方が高精度
- **qwen3-vl-30b（74.7%）から+8.0%の大幅改善** → 30B VL vs 32B VLで8%差
- **全体3位タイ**（qwen3-235b 88%, gpt-oss-120b 88%に次ぐ）
- Baseline/案Bで同率最高、シンプルなプロンプトが効果的（大規模モデルの傾向）
- 案A/案Cでは-1.3%とわずかに低下
- 全プロンプトで非常に安定（81.3%-82.7%、±0.7%）
- Avg Time: 3.7-3.9s
- 空回答なし、エラーなし、安定出力
- qwen3 VLスケーリング: 8B(60.0%) → 30B(74.7%) → **32B(82.7%)**

#### qwen/qwen3-vl-4b（75問）小規模VLモデル

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | **案B: 段階的思考** | **52.0%** | 39/75 | **+2.7%** |
| 2 | 案A: 回答形式強化 | 50.7% | 38/75 | +1.3% |
| 2 | 案C: 日本医療文脈 | 50.7% | 38/75 | +1.3% |
| 4 | Baseline (現行) | 49.3% | 37/75 | - |

**発見**:
- 4B VLモデルで52.0%達成
- **qwen3-4b（非VL、54.7%）より-2.7%低い** → 小規模ではVL版が不利
- qwen3-vl-32b(82.7%)ではVL版がNon-VL(80.0%)を上回ったが、4Bでは逆転
- **qwen3 VLスケーリング**: **4B(52.0%)** → 8B(60.0%) → 30B(74.7%) → 32B(82.7%)
- 案B（段階的思考）でわずかに改善（+2.7%）
- 全プロンプトで安定（49.3%-52.0%、±1.4%）
- 空回答なし、安定出力
- Avg Time: 1.9-2.1s

#### liquid/lfm2.5-1.2b（75問）超小規模モデル

| Rank | プロンプト | 正答率 | 正解数 | ベースライン比 |
|------|-----------|--------|--------|---------------|
| 1 | **案A: 回答形式強化** | **28.0%** | 21/75 | **+12.0%** |
| 2 | Baseline (現行) | 16.0% | 12/75 | - |
| 3 | 案B: 段階的思考 | 13.3% | 10/75 | -2.7% |
| 3 | 案C: 日本医療文脈 | 13.3% | 10/75 | -2.7% |

**発見**:
- **評価した中で最小のモデル（1.2B、1.25GB）**
- 案A（回答形式強化）で28.0%達成、ベースラインから+12.0%改善
- 指示追従は可能（nemotron-3-nanoやfallen-commandのような異常出力はない）
- 1.2Bでも案A（明確なフォーマット指示）により2倍近い精度改善
- API error 1件（Baseline）、空回答なし
- Avg Time: 0.1-0.2s と最速推論
- **モデルサイズと精度**: 1.2B(28%) → 4B(54.7%) → 8B(61.3%) → 14B(73.3%) でサイズとの相関が明確

#### qwen/qwen3-next-80b MLX（75問）⭐合格、MoE Active 3B超軽量・超高速

| Rank | プロンプト | 正答率 | 正解数 | 空回答 | ベースライン比 |
|------|-----------|--------|--------|--------|---------------|
| 1 | **案C: 日本医療文脈** | **85.3%** | 64/75 | 0 | **+18.7%** |
| 2 | 案A: 回答形式強化 | 81.3% | 61/75 | 0 | +14.7% |
| 2 | 案B: 段階的思考 | 81.3% | 61/75 | 0 | +14.7% |
| 4 | Baseline (現行) | 66.7% | 50/75 | 0 | - |

**発見**:
- **80B MoE（Active 3B）で85.3%達成、合格ライン超え⭐**
- **Active 3Bという超軽量アクティブパラメータで合格ラインを大幅超え** → MoEアーキテクチャの効率性
- **推論速度が全モデル中最速クラス**: 案A/案C 0.4s/問（案Bは11.5s CoTのため）
- **Baselineが66.7%と低い** → プロンプトへの依存度が非常に高い（案Cで+18.7%改善）
- **案C（日本医療文脈）が最良** → 日本の診療ガイドライン言及が有効
- メモリ使用量: 84.67GB（MLX版）
- max_tokens=1024で運用（デフォルトのmt=50ではBaselineが冗長出力で失敗する傾向）
- `/no_think`指示適用（モデル名にqwen3を含むため自動付与）
- API error 2件あり（Baseline A14, A47）
- 空回答なし、安定出力
- **コスパ分析**: 84.67GBで85.3% vs qwen3-32b(34.83GB)で80.0% → メモリ2.4倍で+5.3%改善
- **qwen3 MoE比較**: qwen3-next-80b-A3B(85.3%) vs qwen3-235b-A22B(88.0%) → Active 3B vs 22Bで-2.7%のみ

#### qwen/qwen3-235b-a22b Thinking実験（75問）⚠️Thinking ONは逆効果

**条件**: `/no_think`をコメントアウトし、max_tokens=1024で全プロンプト実行

| Rank | プロンプト | 正答率 | 正解数 | API Error | ベースライン比 |
|------|-----------|--------|--------|-----------|---------------|
| 1 | **案B: 段階的思考** | **85.3%** | 64/75 | 0 | **+49.3%** |
| 2 | 案A: 回答形式強化 | 81.3% | 61/75 | 2 | +45.3% |
| 3 | 案C: 日本医療文脈 | 80.0% | 60/75 | 0 | +44.0% |
| 4 | Baseline (現行) | 36.0% | 27/75 | 3 | - |

**Thinking ON vs /no_think 比較**

| プロンプト | Thinking ON (mt=1024) | /no_think (mt=50/200) | 差分 |
|-----------|----------------------|----------------------|------|
| Baseline | 36.0% | 80.0% | **-44.0%** |
| 案A | 81.3% | 86.7% | -5.3% |
| 案B | 85.3% | 88.0% | -2.7% |
| 案C | 80.0% | 86.7% | -6.7% |
| **Best** | **85.3%** | **88.0%** | **-2.7%** |

**重大発見**:
- **Thinking ONは全プロンプトで性能低下。`/no_think`が正解⚠️**
- **Baselineが36.0%まで暴落（-44.0%）**: `<think>`タグ内で全選択肢(a-e)を議論するため、タグ除去後も余計な文字を回答抽出で拾ってしまう
- 案A/案Cも-5〜7%低下、案Bは-2.7%で影響が最小（「答え:」パターンが抽出を助ける）
- API errorが増加（Baseline 3件、案A 2件）：思考トークンでレスポンスが肥大化
- 応答時間が大幅増加：/no_think(1.4s) → Thinking ON(12-27s) で約10-20倍
- **原因分析**: `<think>` タグ内容に全選択肢の文字(a,b,c,d,e)が含まれるため、`re.sub(r'<think>.*?</think>', ...)`で除去後も、タグが閉じないケースや残留テキストから誤抽出
- **結論**: qwen3モデルには`/no_think`を必ず使用すること。Thinking ONではmax_tokens増加のメリットよりも回答抽出汚染のデメリットが大きい

#### medgemma-27b-text-it-mlx 全セクション評価（400問）❌不合格

**評価日**: 2026-02-07
**条件**: Baseline プロンプト, max_tokens=50（セクションAでの最適設定）
**合格基準**: 全400問中75%（300問）以上正解

| Section | 問題数 | 正解数 | 正答率 | 空回答 | 備考 |
|---------|--------|--------|--------|--------|------|
| A | 75 | 57 | **76.0%** ✓ | 0 | 単一文字回答で安定 |
| B | 50 | 27 | 54.0% | 18 | 思考モード発動で空回答多発 |
| C | 75 | 24 | 32.0% | 22 | 臨床シナリオで思考モード頻発 |
| D | 75 | 15 | 20.0% | 44 | 後半ほぼ全問空回答 |
| E | 50 | 26 | 52.0% | 14 | 前半は正常回答 |
| F | 75 | 21 | 28.0% | 23 | 「はい、承知いたしました」型回答多発 |
| **合計** | **400** | **170** | **42.5%** | **121** | **不合格**（合格ラインまで130問不足） |

**発見**:
- **セクションAのみ合格（76.0%）、B-Fは全滅（20-54%）**
- **空回答121件（30.3%）**: max_tokens=50で思考モードに入り回答に到達できない
- **回答未到達パターン3種**:
  1. `<unused94>thought`（medgemma内部思考モード）: 英語で推論開始しトークン消費
  2. `はい、承知いたしました`型: タスク確認で終了
  3. `医学の専門家として、`型: 前置きで終了
- **セクション間の難易度差**: A（知識問題）→ B-F（臨床シナリオ）で回答形式が変化
- **max_tokens=50の限界**: セクションAでは1トークン回答だったが、B-Fでは思考プロセスが起動
- **抽出ロジック改善**: `正解は **X**` パターンの抽出、`<unused94>thought`タグ除去を追加（旧ロジックではB=20%→新54%）
- **総合判定: 不合格**（42.5% / 合格ライン75%）
- **注**: max_tokens=1024での再評価でB-Fも改善する可能性あり（gpt-ossで実証済みの知見）

#### medgemma-27b-text-it-mlx Baseline改変+mt=512 再評価（400問）❌不合格（改善）

**評価日**: 2026-02-07
**条件**: 改変Baselineプロンプト + max_tokens=512
```
System: あなたは医学の専門家です。問題を読み、正解の選択肢をa,b,c,d,eで即答してください。
思考過程の出力は不要です。回答のみを出力してください。
max_tokens: 512
```

| Section | mt=50旧 | mt=512改変 | 改善幅 |
|---------|---------|-----------|--------|
| A | 76.0% (57/75) | 64.0% (48/75) | -12.0% |
| B | 54.0% (27/50) | 74.0% (37/50) | +20.0% |
| C | 32.0% (24/75) | 68.0% (51/75) | +36.0% |
| D | 20.0% (15/75) | 69.3% (52/75) | +49.3% |
| E | 52.0% (26/50) | 72.0% (36/50) | +20.0% |
| F | 28.0% (21/75) | 62.7% (47/75) | +34.7% |
| **Total** | **42.5%** (170/400) | **67.8%** (271/400) | **+25.2%** |

**発見**:
- **総合42.5% → 67.8%と大幅改善（+25.2%）、ただし合格ライン75%には未達**
- **B-Fで平均+32%の劇的改善**: 空回答が完全解消（121件→0件）
- **セクションAは-12%低下**: プロンプト改変（「思考過程の出力は不要」追加）が逆効果。元々単一文字回答で問題なかったため、指示追加が回答精度を下げた
- **セクション間の精度差が縮小**: 旧(20-76%) → 新(62.7-74.0%)と均一化
- **推論速度が向上**: 2.1s → 0.8s/問（max_tokens増加にも関わらず高速化）
- **max_tokensとプロンプトの交互作用**: mt=50+シンプルプロンプトはAに最適、mt=512+指示強化はB-Fに最適。全セクション統一設定では最適解が異なる
- **合格ラインまであと29問**（271/400 → 300/400 が必要）

#### medgemma-27b-text-it-mlx 全4プロンプト+Few-shot比較（セクションA, mt=512）

**評価日**: 2026-02-07
**条件**: オリジナル4プロンプト + 2-shot few-shot（第100回問題3問）+ max_tokens=512

| Rank | Prompt | Accuracy | Correct | Empty | Avg Time |
|------|--------|----------|---------|-------|----------|
| 1 | **Baseline + few-shot** | **76.0%** | 57/75 | 0 | 1.0s |
| 2 | 案C + few-shot | 74.7% | 56/75 | 0 | 1.0s |
| 3 | 案A + few-shot | 70.7% | 53/75 | 0 | 0.9s |
| 4 | 案B + few-shot | 56.0% | 42/75 | 0 | 5.3s |

**参考: few-shotなし（mt=512, オリジナルプロンプト）の結果**:

| Prompt | Accuracy (no few-shot) | Accuracy (few-shot) | 改善幅 |
|--------|----------------------|--------------------|----|
| Baseline | 17.3% (13/75) | **76.0%** (57/75) | **+58.7%** |
| 案A | 57.3% (43/75) | 70.7% (53/75) | +13.3% |
| 案B | — (中断) | 56.0% (42/75) | — |
| 案C | — (中断) | 74.7% (56/75) | — |

**発見**:
- **Few-shotがmedgemmaの思考モード抑制に決定的に有効**: Baselineが17.3%→76.0%（+58.7%）
- Few-shot例が回答形式を誘導し、`<unused94>thought`タグの発動を防止
- Baseline + few-shotが空回答0、応答時間~1.0sと最も安定
- 案Bはfew-shotでも一部思考モードが発生（平均応答時間5.3s）

#### medgemma-27b-text-it-mlx Baseline+Few-shot+mt=512 全セクション評価（400問）❌不合格（改善）

**評価日**: 2026-02-07
**条件**: Baseline + 2-shot few-shot + max_tokens=512

| Section | 1回目(mt=50) | 2回目(mt=512改変) | 3回目(mt=512+few-shot) | 改善(2回目比) |
|---------|-------------|-----------------|---------------------|-------------|
| A | 76.0% (57/75) | 64.0% (48/75) | **76.0%** (57/75) | +12.0% |
| B | 54.0% (27/50) | 74.0% (37/50) | **82.0%** (41/50) | +8.0% |
| C | 32.0% (24/75) | 68.0% (51/75) | 61.3% (46/75) | -6.7% |
| D | 20.0% (15/75) | 69.3% (52/75) | **76.0%** (57/75) | +6.7% |
| E | 52.0% (26/50) | 72.0% (36/50) | **76.0%** (38/50) | +4.0% |
| F | 28.0% (21/75) | 62.7% (47/75) | 64.0% (48/75) | +1.3% |
| **Total** | **42.5%** (170/400) | **67.8%** (271/400) | **71.8%** (287/400) | **+4.0%** |

**総合判定: 不合格（287/400 = 71.8%, 合格ライン75% = 300/400）**

**発見**:
- **3回の評価で42.5% → 67.8% → 71.8%と着実に改善（+29.3%）**
- **合格ラインまであと13問**（287/400 → 300/400 が必要）
- **セクションA/B/D/Eが合格ライン(75%)以上**: A(76.0%), B(82.0%), D(76.0%), E(76.0%)
- **セクションC(61.3%)とF(64.0%)が足を引っ張っている**
- セクションCは2回目(68.0%)より逆に低下(-6.7%)。臨床問題の難易度が高い可能性
- **Few-shot + Baselineがmedgemmaの最適設定**: 思考モード抑制 + 回答形式誘導の両方を実現

#### 考察

1. **モデルによって最適なプロンプトが大きく異なる**
   - **qwen3-235b-a22b-2507 MLX 8bit mt=1024: 案B/案C同率（88.0%）⭐⭐2507版、Baseline+6.7%安定化**
   - **qwen3-235b-a22b: 案Bが最良（88.0%）⭐⭐最高スコア、GPT-4超え**
   - **gpt-oss-120b MLX 8bit mt=1024: 案Aが最良（92.0%）⭐⭐⭐全モデル最高スコア**
   - **gpt-oss-120b GGUF MXFP4 mt=1024: 案Cが最良（90.7%）⭐⭐⭐**
   - **qwen3-next-80b mt=1024: 案Cが最良（85.3%）⭐MoE Active 3B、超高速0.4s**
   - **llama-3.3-swallow-70b: Baseline/案B同率（81.3%）⭐日本語ファインチューン**
   - **qwen3-32b: Baselineが最良（80.0%）⭐GPT-4レベル達成**
   - mistral-large-2407: Baseline/案A/案C同率（77.3%）✓合格、123B
   - medgemma-27b: シンプルなBaselineが最良（76.0%）✓合格
   - mistral-small: Baselineが最良（76.0%）✓合格
   - **qwen3-vl-32b: Baseline/案B同率（82.7%）⭐合格、全プロンプトで合格**
   - qwen3-vl-30b: 全プロンプト同等（74.7%）惜しい
   - **qwen3-14b: 案A/案Bが最良（73.3%）惜しい、+5.3%改善**
   - **gpt-oss-20b mt=1024: 案Aが最良（77.3%）✓合格、mt=50では32%→mt=1024で+45.3%**
   - llama-3.3-70b-instruct: 全プロンプト同等（68.0%）安定出力
   - qwen3-vl-8b: Baseline/案B/案C同等（60.0%）ChatGPT超え
   - ezo2.5-gemma-3-12b: 案Cが最良（60.0%）🇯🇵EZO日本語FT、ChatGPT超え
   - phi-4: 回答形式強化が最良（56.0%）+12%改善
   - internvl3_5-8b: Baselineが最良（54.7%）高速
   - qwen3-4b: 日本医療文脈が最良（54.7%）
   - qwen3-vl-4b: 案Bが最良（52.0%）非VL版(54.7%)より-2.7%低い
   - qwen3-8b: 案Aが最良（61.3%）ChatGPT超え、案B壊滅(6.7%)
   - gemma-3-27b: 案Bが最良（74.7%）惜しい、合格ラインまで0.3%
   - shisa-v2-llama3.3-70b: 案A/案Bが最良（61.3%）🇯🇵Shisa日本語FT、ベースより低下
   - llama-3.1-swallow-8b: 案Bが最良（53.3%）🇯🇵Swallow 8B
   - elyza-jp-8b: 回答形式強化が最良（44.0%）🇯🇵日本語特化
   - medgemma-4b: 回答形式強化が最良（29.3%）
   - lfm2.5-1.2b: 案Aが最良（28.0%）最小モデル(1.2B)、指示追従可能
   - fallen-command-a-111b: 全問でd,e含む複数選択肢列挙（1.3%）⚠️指示追従不能
   - nvidia/nemotron-3-nano: 全問で複数選択肢列挙（0-1.3%）⚠️指示追従不能
   - glm-4.6v-flash-mlx: 日本語非対応で評価不能（2.7%）⚠️

2. **qwen3のThinking ONは逆効果**
   - qwen3-235b-a22b: /no_think(88.0%) → Thinking ON(85.3%) で**-2.7%**低下（Bestスコア比較）
   - Baselineでは80.0% → 36.0%と**-44.0%**の壊滅的低下
   - `<think>`タグ内の選択肢議論が回答抽出を汚染するため
   - **結論**: qwen3には`/no_think`を必ず使用

3. **max_tokensの重要性（reasoningモデルでは最重要パラメータ）**
   - gpt-oss-120b GGUF: mt=50(29-33%) → mt=200(82-85%) → **mt=1024(85-90.7%)** ← 最重要発見
   - gpt-oss-120b MLX 8bit: mt=50(26-30%) → **mt=1024(85-92.0%)** ← **全モデル最高スコア⭐⭐⭐**
   - gpt-oss-120b: reasoningトークンがmax_tokensを消費 → contentに回答が出力されない
   - mt=50: 空回答43-49件、mt=200: 4-6件、**mt=1024: 0件**（完全解消）
   - gpt-oss-20b: mt=50(26-32%) → mt=200(68% 案Bのみ) → **mt=1024(74-77.3%)** ✓合格ライン達成
   - medgemma-27b: max_tokens=200で逆に複数選択肢を出力し抽出失敗
   - **結論**: reasoningモデルにはmax_tokens≧1024を推奨、非reasoningモデルは50-200で十分

3. **大規模医療特化モデルはシンプルなプロンプトで十分**
   - medgemma-27bは余計な制約を加えると性能低下

4. **汎用モデルは思考スペースが必要**
   - gpt-oss-20bは段階的思考で+36%の劇的改善

5. **小規模モデルは文脈情報が有効**
   - qwen3-4bは日本のガイドラインへの言及で+2.7%改善

#### v1 vs v2 プロンプト比較

| モデル | v1（試験名あり） | v2最良 | 差分 |
|--------|-----------------|--------|------|
| **qwen3-235b-a22b-2507** (MLX 8bit mt=1024) | - | **88.0%** (案B/案C) | **新規** ⭐⭐Baseline+6.7%安定化 |
| **qwen3-235b-a22b** | - | **88.0%** (案B) | **新規** ⭐⭐最高スコア |
| **gpt-oss-120b** (MLX 8bit mt=1024) | - | **92.0%** (案A) | **新規** ⭐⭐⭐全モデル最高スコア |
| **qwen3-next-80b** (MLX mt=1024) | - | **85.3%** (案C) | **新規** ⭐MoE A3B、超高速0.4s |
| **gpt-oss-120b** (GGUF MXFP4 mt=1024) | - | **90.7%** (案C) | **新規** ⭐⭐⭐mt=1024で空回答解消 |
| **llama-3.3-swallow-70b** | - | **81.3%** (Baseline/案B) | **新規** ⭐日本語FT+13.3% |
| **qwen3-32b** | - | **80.0%** (Baseline) | **新規** ⭐GPT-4レベル |
| **qwen3-vl-32b** | - | **82.7%** (Baseline/案B) | **新規** ⭐合格、VL版+2.7% |
| **mistral-large-2407** | - | **77.3%** (Baseline/案A/案C) | **新規** ✓合格 123B |
| **qwen3-14b** | - | **73.3%** (案A/B) | **新規** 惜しい |
| **gpt-oss-20b** (mt=1024) | 28.0% | **77.3%** (案A) | **+49.3%** ⭐✓合格 |
| llama-3.3-70b-instruct | - | 68.0% (Baseline) | **新規** 安定出力 |
| qwen3-vl-8b | - | **60.0%** (Baseline) | **新規** ChatGPT超え |
| internvl3_5-8b | - | 54.7% (Baseline) | **新規** 高速推論 |
| ezo2.5-gemma-3-12b | - | 60.0% (案C) | **新規** 🇯🇵日本語FT+5.3% |
| phi-4 | - | 56.0% (案A) | **新規** +12%改善 |
| medgemma-4b@bf16 | 25.3% | 29.3% (案A) | +4.0% |
| medgemma-27b | 73.3% | 76.0% (Baseline) | +2.7% |
| mistral-small | - | 76.0% (Baseline) | **新規** ✓合格 |
| qwen3-vl-30b | - | **74.7%** (案A/B/C) | **新規** 惜しい |
| gemma-3-27b | - | 74.7% (案B) | **新規** 惜しい、medgemmaと1.3%差 |
| qwen3-vl-4b | - | 52.0% (案B) | **新規** 非VL版(54.7%)より-2.7% |
| qwen3-4b | 57.3% | 54.7% (案C) | -2.6% |
| qwen3-8b | - | 61.3% (案A) | **新規** 案B壊滅(6.7%) |
| shisa-v2-llama3.3-70b | - | 61.3% (案A/B) | **新規** 🇯🇵ベースより-6.7%低下 |
| llama-3.1-swallow-8b@bf16 | - | 53.3% (案B) | **新規** 🇯🇵Swallow 8B |
| elyza-jp-8b | - | 44.0% (案A) | **新規** 🇯🇵日本語特化 |
| fallen-command-a-111b | - | 1.3% (全同) | **新規** ⚠️指示追従不能、3bit量子化 |
| nvidia/nemotron-3-nano | - | 1.3% (案B) | **新規** ⚠️指示追従不能 |
| lfm2.5-1.2b | - | 28.0% (案A) | **新規** 最小モデル(1.2B) |
| glm-4.6v-flash-mlx | - | 2.7% (案A/C) | **新規** ⚠️評価不能 |

**発見**:
- gpt-ossシリーズは段階的思考（案B）で劇的改善。max_tokens増加と思考プロセス誘導が効果的
- gpt-ossスケーリング効果(mt=1024): **20B(77.3%)** → 120B GGUF(90.7%) → **120B MLX(92.0%)**
- **gpt-oss-120b max_tokens実験**: mt=50で空回答多発(26-33%) → mt=200で82-85% → **mt=1024で85-92%**。reasoningモデルではmax_tokensが精度を支配
- **gpt-oss-120b MLX 8bit mt=1024: 案A(92.0%)⭐⭐⭐全モデル最高スコア**
- **GGUF vs MLX比較(mt=1024)**: MLX 8bit(92.0%) > GGUF MXFP4(90.7%) → MLXが+1.3%高いが、メモリ2倍(124GB vs 63GB)・速度で劣る(2.1s vs 1.3s)
- **日本語ファインチューニングの効果**: llama-3.3-70b(68.0%) → **swallow-70b(81.3%)** で**+13.3%**向上
- **Swallowスケーリング効果**: 8B(53.3%) → 70B(81.3%) で**+28.0%**向上。日本語FT 8Bでもelyza-jp-8b(44.0%)を+9.3%上回る
- **EZO日本語FT効果**: gemma-3-12b(54.7%) → ezo2.5(60.0%) で**+5.3%**向上。案C（日本医療文脈）が最適
- **gemmaスケーリング効果**: 12B(54.7%) → 27B(74.7%) → medgemma-27b(76.0%)。医療FTの効果はわずか+1.3%
- **medgemma-27b 全セクション評価**: A(76.0%) / B(54.0%) / C(32.0%) / D(20.0%) / E(52.0%) / F(28.0%) → **総合42.5%で不合格**。セクションAのみ合格、B-Fでは思考モードに入りmax_tokens=50で回答未到達（空回答121/400=30.3%）
- **medgemma-27b 最適設定(Baseline+few-shot+mt=512)**: A(76.0%) / B(82.0%) / C(61.3%) / D(76.0%) / E(76.0%) / F(64.0%) → **総合71.8%で不合格（あと13問）**。Few-shotが思考モード抑制に決定的に有効（Baseline: 17.3%→76.0%）
- **Shisa日本語FTは逆効果**: llama-3.3-70b(68.0%) → shisa-v2(61.3%) で**-6.7%**低下。日本語FTが必ずしも改善にならない
- **llama-3.3-70b 日本語FT比較**: Swallow(81.3%) >> base(68.0%) > Shisa(61.3%) → FT手法で20%の差
- **Mistralスケーリング効果**: small(76.0%) → Large 123B(77.3%) で**+1.3%**のみ。コストパフォーマンスは低い
- medgemmaではv2の方が高精度。試験名なしでも問題なし
- qwen3-4bではv1の方が高精度で、ベンチマーク汚染の影響がある可能性
- qwen3シリーズのスケーリング効果: 4B(54.7%) → 8B(61.3%) → 14B(73.3%) → 30B(74.7%) → 32B(80.0%) → **VL-32B(82.7%)** → **Next-80B-A3B(85.3%)** → **235B(88.0%)** で明確なスケーリング
- **qwen3 MoE効率性比較**: qwen3-next-80b(Active 3B, 85.3%) vs qwen3-235b-a22b(Active 22B, 88.0%) → Active パラメータ7.3倍の差でわずか-2.7%、MoEの効率性が顕著
- **qwen3 VL vs Non-VL比較**: 32B: VL(82.7%) > Non-VL(80.0%) で+2.7%、4B: VL(52.0%) < Non-VL(54.7%) で-2.7% → 大規模ではVL有利、小規模では不利

---

## 次のステップ候補

- [x] medgemma-27bで全セクション(A-F)評価 → **不合格**（42.5%、詳細は下記）
- [ ] より大規模モデル(30B+)の評価
- [ ] 年度別比較(2018-2022)
- [ ] カテゴリ別分析（神経科、放射線科など）
- [x] プロンプト最適化の効果検証 → 比較スクリプト作成済み

---

## 変更履歴

### 2026-02-07（セッション3）
- **medgemma-27b 全4プロンプト+Few-shot比較** 完了（セクションA, mt=512）
  - Baseline+few-shot: 76.0%（最良）、案C+few-shot: 74.7%、案A+few-shot: 70.7%、案B+few-shot: 56.0%
  - Few-shotが思考モード抑制に決定的に有効（Baseline: 17.3%→76.0%, +58.7%）
- **medgemma-27b Baseline+Few-shot+mt=512 全セクション(A-F)評価** 完了
  - 総合: 287/400 (71.8%) → **不合格**（合格ライン75%まであと13問）
  - A(76.0%), B(82.0%), C(61.3%), D(76.0%), E(76.0%), F(64.0%)
  - 3回の評価で42.5% → 67.8% → 71.8%と着実に改善
  - セクションC/Fが足を引っ張り合格ラインに届かず

### 2026-02-07（セッション2）
- **qwen3-next-80b** (MLX, 80B MoE Active 3B) 評価完了
  - Best: 85.3%（案C: 日本医療文脈）、合格ライン超え
  - max_tokens=1024で全プロンプト実行、超高速0.4s/問
  - Active 3Bで85.3%はMoEアーキテクチャの効率性を示す
  - Baselineが66.7%と低く、プロンプト依存度が高い（案Cで+18.7%改善）
- プロット更新（scatter, pareto+budget に qwen3-next-80b 追加、~85GBバジェットティア追加）
- **medgemma-27b-text-it-mlx 全セクション(A-F)評価** 完了
  - 総合: 170/400 (42.5%) → **不合格**
  - セクションA(76.0%)のみ合格、B-F(20-54%)は全滅
  - 空回答121件(30.3%): 思考モード・前置き型回答がmax_tokens=50を消費
  - 回答抽出ロジック改善: `正解は **X**` パターン認識、`<unused94>thought`タグ除去追加
- **medgemma-27b Baseline改変+mt=512 再評価** 完了
  - プロンプトに「思考過程の出力は不要です。回答のみを出力してください。」追加、max_tokens=512
  - 総合: 42.5% → **67.8%**（+25.2%）、ただし合格ライン75%には未達
  - B-Fで空回答完全解消、セクションAは-12%低下（プロンプト改変が逆効果）

### 2026-02-06
- **qwen3-235b-a22b-2507** (MLX 8bit, Instruct版) 評価完了
  - Best: 88.0%（案B/案C同率）、旧版と同率だがBaseline+6.7%安定化
  - 全プロンプト86.7-88.0%と極めて安定（旧版は80.0-88.0%）
- **qwen3-235b-a22b Thinking実験** 完了
  - Thinking ON は全プロンプトで性能低下、`/no_think`が正解
  - Baselineで80.0% → 36.0%と壊滅的低下
- **gpt-oss-120b** MLX 8bit max_tokens=1024 評価完了
  - Best: 92.0%（案A）、全モデル・全条件で最高スコア
- **gpt-oss-120b** GGUF MXFP4 max_tokens実験（50/200/1024）完了
  - max_tokensが精度を支配する重大発見
- **gpt-oss-20b** max_tokens=1024 評価完了
  - Best: 77.3%（案A）、mt=50の32%から合格ラインへ
- **mistral-large-instruct-2407** 評価完了: 77.3%合格
- **qwen3-vl-32b** 評価完了: 82.7%合格、VL版がqwen3-32b超え
- **gemma-3-27b** 評価完了: 74.7%惜しい
- **shisa-v2-llama3.3-70b** 評価完了: 61.3%、ベースより低下
- **qwen3-8b** 評価完了: 61.3%、案B壊滅(6.7%)
- **qwen3-vl-4b** 評価完了: 52.0%
- **lfm2.5-1.2b** 評価完了: 28.0%、最小モデル
- **fallen-command-a-111b** 評価失敗: 指示追従不能
- プロット3種作成（scatter, scaling, pareto+budget）
- glm-4.7-flash 詳細診断実施

### 2026-02-02
- v2プロンプト比較実験開始
- evaluate_prompt_comparison.py 作成
- 初期モデル群評価（medgemma-27b, qwen3-4b, gpt-oss-20b等）

### 2026-02-01
- IgakuQA評価パイプライン構築
- v1プロンプトでの初回評価
- GitHubリポジトリ作成
